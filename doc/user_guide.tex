\documentclass[11pt,preprint]{aastex}
\usepackage[T1]{fontenc}

\usepackage{lmodern}
\usepackage{etoolbox}

\newcommand{\param}[2]{\medskip\noindent\textbf{\texttt{#1}}: ({\tt #2}) }

% If the \public flag is set, `private` environments and \todo's will not be rendered
\ifdef{\public}{
\newcommand{\todo}{}
\newcommand{\future}{}
\newenvironment{private}{\comment}{\endcomment}
}
{
\newcommand{\todo}[1]{{\bf TODO: #1}}
\newcommand{\future}[1]{{\it Future: #1}}
\newenvironment{private}
    {\textbf{Not public:}\\
    \begin{tabular}{|p{0.9\textwidth}|}
    \hline\\
    }
    {
    \\\\\hline
    \end{tabular}
    }
}

\newcommand{\fdir}[1]{\noindent\makebox[4in][l]{#1}}

\begin{document}
\title{ABACUS User Guide}
\author{\today}

\section{Introduction to Abacus}

Abacus is a cosmology N-body code intended for large cosmological
simulations with high-force accuracy.  It utilizes fast near-field
computations using GPUs and AVX instructions, as well as a novel
method for solving the far field.

The Abacus computation is organized into a 3-dimensional grid of
cells.  Particles belong to cells.  The near-field force is computed
within a cell and its near neighbors.  Set finding also happens
within a fixed maximum number of cells.  The far-field force is
computed from the multipole moments of particles in the cells.

Abacus is built to run out-of-core, so that the particles are read
only once per full time step.  The computation occurs through a
pipeline that takes a 1-dimensional sweep through the simulation
volume.  The plane of cells perpendicular to the sweep direction
are called a slab.  This is the $x$ direction in the simulation
units.

Abacus evolves through a series of states.  Every full time step
reads a state, evolves the particles, and then writes a new state.
The states are the restart files.  Because of the leapfrog time
evolution, the states are generally not time-synchronous between
the positions and velocities: the velocities will require an
additional kick by the listed kick factor to be synchronous with
the positions.

Because of this leapfrogging, the state files are generally not
intended for long-term storage of the outputs.  Instead, Abacus
generates output files during full steps.  Time slice outputs will
occur at the redshift of the ``read'' state (so the full time step
will be shortened so as not to overshoot the requested time slice).
Light cone outputs are interpolated to the time when the past light
cone from an observer crosses the particle or the coevolution set.

Within a full step, microstepping may occur.  This means that
particles in higher density regions with shorter dynamical times
will use shorter leapfrog steps.  However, to avoid needing to load
the particles more than once from disk, the regions requiring finer
time steps are identified on the fly so that all of the microsteps
occur at once.  The regions requiring mutually finer time resolution
of the forces are known as coevolution sets.  By definition, all
particles outside of a given coevolution set have forces that are
sufficiently constant over the full step as to be applied with only
the top-level leapfrog.  Note that one can always pick a short
enough full time-step to satisfy one's accuracy criterion for this.
Inside the coevolution set, a variety of time steps may be used.

In addition to their utility in time stepping, co-evolution sets
also define the maximum boundaries of our halo and subhalo finding.
In other words, halo finding never extends beyond the set boundary.
Finally, co-evolution sets are cohesive for light-cone outputs: we
will always output a snapshot of a co-evolution set at the ``read''
state epoch, but shifting the center of mass position and velocity
drift to the exact epoch when the past light cone sweeps by.

Coevolution sets are found by performing friends-of-friends with a
fairly large linking length.

\begin{private}
\section{Coevolution Sets and Group Finding}

More on co-evolution sets.
\end{private}

\begin{private}
\section{Light Cone Outputs}

Describe light cones here.
\end{private}

\section{Zel'dovich Initial Conditions}

The zeldovich code creates displacement vectors appropriate to 
the given power spectrum.  It normalizes and scales the input
power spectrum as instructed.  Interpolation to the needed wavenumbers
is done by spline, so one should input a reasonably fine grid.
Modes with wavenumbers $|\vec{k}|$ above the Nyquist frequency are
set to zero to avoid grid directionality artifacts.
The code is available as a separate module, along with extensive documentation,
at \url{https://github.com/abacusorg/zeldovich-PLT}.

The zeldovich code can also use the particle lattice eigenmodes
instead of the usual continuum $\hat{k}$ modes to eliminiate
the vorticity and decaying modes that usually enter on small scales.
The code can also fudge the initial mode amplitudes to account for
different growth rates as a function of $\vec{k}$, with the hope of
seeding non-linear evolution with the correct linear displacements.
These are the PLT (``Particle Linear Theory'') features of the code.

Displacement vectors are output along with the grid position (as
integers).  The abacus IC loader maps this information to positions
and velocities.  Recall that in $\Omega_m=1$, the comoving displacement
vector is the same as the velocity in redshift-space displacement
units.  In other cosmologies, the scaling is $f = d\ln D/d\ln a$.

Displacement vectors are computed by multiplying by $\vec{ik}$ in
Fourier space, rather by differencing in configuration space.  In
addition, we compute the density field grid itself, although this
is not output by default.  The 4 real FFTs are done by pairing into
2 complex FFTs.

When using PLT features, the velocities are not simply a rescaling
of the displacements and have to be calculated in Fourier space.
We pack the three velocity components into another 2 complex FFTs.
In this case, the outputs will include the displacement and velocity.

Unfortunately, the zeldovich code labels the slab direction by $z$.
It flips zyx to xyz as it outputs.

The zeldovich code is built to work out-of-core.  It splits the 3-d
computational volume in two directions, $y$ and $z$, into NumBlocks
each.  It then performs the 3-d FFT in two parts, first loading
slabs in $y$ and doing the $z$ FFT (and establishing the Hermitian
symmetries), and then considering slabs in $z$, doing the $xy$ FFTs,
and outputing in $z$ order (which becomes the abacus slab order).
Between the two parts, we write the slabs to disk in $yz$ blocks.
In this way, we require only $2/{\rm NumBlocks}$ fraction of the
problem to be in memory.

Currently a fast 1-d FFT routine computes of order 500 Mcells/sec,
which implies a memory access requirement of 8 GB/sec.  Hence, the
zeldovich code is likely to be I/O limited.  However, total I/O is 
only 32+32+18 bytes per particle.  This is comparable to the I/O 
of a single abacus step.  So unless the disk latency is noticeable,
we expect the run-time to be comparable to one abacus step.

\section{Parameter File Description}

The parameter file holds values that should be constant throughout the
simulation.  In principle, one could change the file when restarting
from a state, but behavior is not guaranteed.  Still, this could be
useful for adjusting output parameters, for example.

Values that change with time as the simulation evolves or that are 
calculated by the abacus code or cosmology module are in the state
file.

Both headers are incorporated into output file headers. 

\subsection{Basic Simulation Parameters}

\param{NP}{long long int} The number of particles in the simulation.
Note that math is allowed in inputs to GenParam, so you might opt to
set this to a perfect cube, e.g., to 1024**3.

\param{CPD}{int} The cells per dimension.  This number must be odd.
Should be a number with small prime factors.  Small numbers of distinct factors are good too.
See the \texttt{Abacus/choose\_cpd.py} utility for guidance on choosing this.

\param{BoxSize}{float} The size of the box in comoving physical units.

\param{hMpc}{int} =1 if we're using Mpc/h units.  =0 if Mpc units.

\param{TimeStepAccel}{float} Time-step parameter based on accelerations.

\param{TimeStepDlna}{float} Maximum $\Delta(\ln a)$ allowed for a full step.
For example, 0.02 means at least 50 steps per $e$-folding of the scale factor.
DJE recommends at least 20 steps per $e$-fold for most applications, or .05.

\param{FinalRedshift}{float} The final redshift of the simulation.
If not supplied, then we use the lowest redshift TimeSlice output.
However, one might want to run further to complete light cones.

\param{StateIOMode}{string} One of "normal", "overwrite", "stripe", or "slosh".  If "overwrite", ReadDirectory and WriteDirectory are internally set to the ReadDirectory.
This saves space, and can be particularly useful for a ramdisk.

\param{Conv\_IOMode}{string} One of "normal", "overwrite", "stripe", or "slosh".  Just as \texttt{StateIOMode}, but for the multipoles and Taylors.

\begin{private}
\subsection{Set and Group Finding}

\param{GroupRadius}{int} The maximum size of a coevolution set, 
in units of cells (i.e., BoxSize/cpd).
\end{private}

\subsection{Memory Allocation and other Execution Parameters}

\param{NumSlabsInsertList}{float} The amount of space to allocate for
the insert list.  This is supplied as a multiple (not necessarily integral)
of np/cpd particles, i.e., the average number of particles per slab.
The default is 2.

\param{NumSlabsInsertListIC}{float} The amount of space to allocate
for the insert list in the initial ingestion of IC particles.  Here
we typically have much less memory required elsewhere in the code, so
this number can be larger than NumSlabsInsertList.
Again, this is supplied as a multiple of np/cpd particles, i.e., the
average number of particles per slab.  Choosing 0 causes this parameter
to be set to cpd, so that the insert list can contain all of the particles
in the simulation.  This is appropriate for small problems where the
particles aren't sorted into slabs in the IC file.  For large problems,
one should use at least 4.  Default is 4.

\param{MAXRAMMB}{int} The maximum amount of RAM to use for internal slab 
allocations.  Default is to detect the total amount of RAM.  Internally,
the code will load slabs until it reaches half this amount.  The other
half is saved for incidental allocations, like source/sink pencils.

\param{OMP\_NUM\_THREADS}{int} Number of OpenMP threads.  The default, 0, does not modify the system value
(set by the \$OMP\_NUM\_THREADS environment variable; if not given, default is to use all cores).
Negative values use that many fewer than the max.
For example, a value of $-4$ will use all but 4 cores.  If using OMP\_PLACES, this
number should be equal to the total number of ``places'' (i.e.~cores) specified.

\param{OMP\_PLACES}{string} Specifies on which cores to place OpenMP
threads.  This is useful for keeping the OpenMP threads on separate
cores from the non-OpenMP threads (like the IO and GPU threads).
For example, to use the first 8 cores on each of two sockets that have
12 cores each, use ``\{0\}:8,\{12\}:8''.  Syntax is "\{start\}:count".
The total number of places (16 in this example) should equal the OMP\_NUM\_THREADS value.
To have any effect, this parameter must be used in conjunction with
OMP\_PROC\_BIND, which tells OpenMP how to assign threads to the
``places'' (cores) listed here.  
The default is an empty string, meaning no explicit core assignemnts.
In this case, the kernel scheduler will just do its best to keep threads
separate.

Note that OpenMP offers no C/C++ interfaces to this
functionality, so OMP\_PLACES is set as an environment variable by
the Python wrapper.  This means that these parameters will have no
effect if the singlestep binary is invoked outside the Python wrapper.
The resulting thread-to-core assignments can be seen at the top of the log
file.
\todo{These are the only parameters need the Python wrapper to work.
Can we change that?}

\param{OMP\_PROC\_BIND}{string} Specifies how to assign OpenMP
threads to the ``places'' (cores) that were specified in OMP\_PLACES.
A value of "spread" seems to have the desired behavior of dividing
threads betweeen sockets when on a NUMA machine.  The default (empty
string) means do not bind threads to places.  The resulting placements
can be seen at the top of the log file.

\param{IOCores}{list of two ints} The cores to which to bind the IO thread(s).
The default ([-1, -1]) means do not bind to a core.  This list should always
have two elements (one for each IO thread); this does not mean that two
IO threads will be started though.  That is determined by SecondIOThreadDirs.
It usually seems advantageous to give each IO thread its own core.

\param{GPUThreadCoreStart}{list of ints} The cores on which to start placing
the threads that dispatch work to the GPUs.  NGPUThreadCores sets the count of cores
to use immediately following these ``start cores'' to use.  There are DirectBPD*NGPU GPU threads
total.  ``DirectBPD'' is ``buffers-per-device'' and is set by ``./configure --with-direct-bpd=BPD'' (default 3).
The GPU threads are in charge of copying data to and from pinned memory (the GPU staging area);
usually three threads per GPU can keep the GPUs fed.  These threads can be on the same core,
but it usually incurrs 20\% performance hit to the effective GPU rate (because the copies get slower).
This list of cores should be NGPU long; the idea is to keep all the threads that
communicate with a given GPU on the same socket.  On a 24 core system divided into two sockets,
something like "[9, 21]" and NGPUThreadCores = 3 will give each GPU thread its own core (assuming DirectBPD = 3
and NGPU = 2).  "[11, 23]" and NGPUThreadCores = 1 would give 3 threads to each core.  In both
cases, threads belonging to the same GPU will be on the same socket.  These
cores should be separate from the OpenMP and IO cores specified by OMP\_PLACES
and IOCores.  The default of all -1 means do not bind GPU threads to cores.

\param{NGPUThreadCores}{int} The number of cores following each core specified in GPUThreadCoreStart
that will be available for GPU threads.  Default of -1 means do not bind threads to cores.

\param{Conv\_OMP\_NUM\_THREADS}{int} Same as OMP\_NUM\_THREADS, but for the Convolution step.
A value of $-1$ is usually appropriate, since we only need to reserve one core for the IO thread.

\param{Conv\_OMP\_PLACES}{string} Same as OMP\_PLACES, but for the Convolution step.
You don't need to pay much attention to sockets here; the main thing is to make sure
the OpenMP places don't overlap with Conv\_IOCore.  Something like ``\{0\}:23'' is fine if
Conv\_IOCore is 23.

\param{Conv\_OMP\_PROC\_BIND}{string} Same as OMP\_PROC\_BIND, but for the Convolution step.
Either ``spread'' or ``close'' should be fine.

\param{Conv\_IOCore}{int} Same as IOCores, but for the Convolution step.  Also, we only have
one convolution IO thread presently, so this is just an int (not a list).  The important
thing is to keep this separate from the cores specified in Conv\_OMP\_PLACES.



\subsection{Far-field Parameters}

\param{Order}{int} The multipole order to use for the far-field.  Typical
choices would be 8 or 16.  One really should not use low orders such
as 2!  Any integer between 2 and 16 is allowed.

\param{DerivativeExpansionRadius}{int} The number of periodic replicas
to sum over explicitly before switching to the asymptotic value.
\todo{Suggest a good value!}
\todo{Is this the right definition?}

\param{ConvolutionCacheSizeMB}{int} The size of the convolution cache.  Should fit within L2 or L3,
so this value will be around ~10 MB.  Default is to detect the L3 cache size.

\param{DerivativesDirectory}{string} The directory where the derivative
files are stored.  One expects to reuse derivatives between
simulations, rather than regenerating them each time.  The derivative
files are about 1/16 the size of the multipoles, so one would like
the I/O rate on this device to be 1/32 of the rate of the device
for the multipoles and Taylors.  Because the derivative files are
not slab-ordered, the latency issue is much reduced.  So in many
cases, the derivatives can be stored on a normal hard disk.  However,
the convolution is presently almost entirely IO bound, so it's not a
bad idea to put the derivatives on the fastest device available.
\todo{We could add a second IO thread to handle the derivatives and
overlap their IO with the M/T.}


\subsection{Near-field Parameters}

\param{NearFieldRadius}{int} The radius of cells to be solved in the
near-field.  Radius 1 means that each cell is acted on by itself and
26 neighbors.  Radius 2 means 124 neighbors.  One really should not
use radius 0!

\param{SofteningLength}{float} The softening length used in the near-field
force, in the same units as BoxSize. For example, .05 would yield a 50 kpc/h comoving softening length,
if BoxSize is given in comoving Mpc/h.
This is the Plummer-equivalent length and may be internally converted to a different
value depending on the softening technique being used.  That is stored in the state as
SofteningLengthInternal.  The conversion ensures that the minimum orbital period is always
$\sqrt{GM/\epsilon}$ (i.e.~the minimum Plummer orbital period) independent of the
softening technqiue being used.

\param{DirectNewtonRaphson}{int} Choice of 1 means that we use a
Newton-Raphson step to improve the precision of the forces.
\future{Is this really a choice that we want to offer?}
\todo{This option appears to have no effect in the current code.}

\param{GPUMinCellSinks}{int} Number of particles in a cell below which its forces will be computed on the CPU, not GPU.
\todo{Currently disabled.  Check functionality in the group-finding code.}

\future{We will eventually have parameters for such choice as trees, etc.}

\subsection{State Directories}
The following defaults are set in the \texttt{directory.def} file.  You usually want to ``\texttt{\#include directory.def}''
just after setting SimName in your \texttt{abacus.par2} file.

\param{WorkingDirectory}{string} Unless countermanded by the choices below,
this is where the states will be created.  This should be a large disk capable
of fast sequential IO, such as a large RAID array.  The default is read from
the \$ABACUS\_TMP environment variable.

\param{ReadStateDirectory}{string} The name of the read state directory.
This defaults to ``WorkingDirectory/read''.

\param{WriteStateDirectory}{string} The name of the write state directory.
This defaults to ``WorkingDirectory/write''.

\param{PastStateDirectory}{string} The name of the past state directory.
This defaults to ``WorkingDirectory/past''.

\param{MultipoleDirectory}{string} The name of the multipole directory.
This should be a fast, low-latency disk, like an SSD.  The default is
``\$ABACUS\_SSD/multipoles'', where \$ABACUS\_SSD is an enviroment variable.

\param{TaylorDirectory}{string} The name of the Taylors directory.
This should be a fast, low-latency disk, like an SSD.  The default is
``\$ABACUS\_SSD/taylor'', where \$ABACUS\_SSD is an enviroment variable.

\subsection{Initial Conditions}

\param{InitialRedshift}{float} The initial redshift of the simulation.

\param{LagrangianPTOrder}{int} Instruction for how to use the initial 
Zel'dovich dispacements.  =1 for Zel'dovich, =2 for 2LPT, =3 for 3LPT.
The higher-order Lagrangian Perturbation Theory is performed by using
Abacus to compute forces, which then get scaled to the LPT displacements
at the given redshift.  See doc/lpt.pdf for more detail.

LagrangianPTOrder >= 2 may involve re-reading the IC files during the last LPT step (probably Abacus step 2),
because the velocities are used as scratch space in our in-place LPT scheme.  If the ICs included velocities
(e.g.~because ZD\_qPLT was turned on the Zel'dovich code), then we re-read the IC files to recover these velocities.

2LPT is tested and works well; 3LPT is implemented but does not give the expected results.

\param{InitialConditionsDirectory}{string} The directory where the 
initial condition files will be found.  This is also where the zeldovich
code with write its outputs.
This defaults to ``\$ABACUS\_PERSIST/ic'', where \$ABACUS\_PERSIST is an
environment variable. We try to avoid fragmentation of the filesystem
where the main state is stored (\$ABACUS\_TMP), hence the use of \$ABACUS\_PERSIST.

These files must be named ``IC\_0'', ``IC\_1'', etc.  The number
indicates the slab number; files $N-1$, $N$, and $N+1$ will be read
before slab $N$ is finished. File $N=0$ is the first file read;
slab 1 is the first slab finished.  This means that the particles
in slab $N$ must appear no later than file $N+1$.  However, if the
particles appear in a much earlier file, then the insert list will
become very large.  This is acceptable if the simulation fits entirely
into memory.  For large sims, we expect that the particles will be 
sorted into slabs, with no more than $\pm 1$ tolerance.  This tolerance
is likely sufficient, e.g., for Zel'dovich displacements, so that particles
can be placed in slab files by their grid position rather than their
true initial position.

If a larger tolerance is needed, one can give FINISH\_WAIT\_RADIUS a larger value, like 2, inside Abacus.

\param{ICFormat}{string} The format of the IC files.  Current formats
are: `RVdouble', `RVZel', `RVdoubleZel', `Heitmann', and `Zeldovich'.
The binary formats are documented at \url{https://github.com/abacusorg/zeldovich-PLT}.

\param{ICPositionRange}{float} The size of the periodic box for the 
IC positions, in the units supplied in the IC file.  For example, 
ICPositionRange of 1 means that the positions are all 0..1.  A choice
of 0 causes this parameter to be set equal to BoxSize.

\param{ICVelocity2Displacement}{float} The conversion factor by which
to multiply the supplied velocities so as to convert them to redshift-space
comoving displacements, in the same units as the supplied positions (see
ICPositionRange) and at the initial redshift.  A factor of 1.0 is appropriate
for ICs received from the zeldovich code.  A factor of -1 will automatically
handle proper km/s.

\param{FlipZelDisp}{int} If $\ge0$, reverse the sign of the displacement, if we're using a Zel'dovich-style IC format.
Useful for debugging the second-order forces used in 2LPT.

\subsection{Output Parameters}

\param{SimName}{string} The name of the simulation, intended to be unique
for each simulation.  This is used both as the primary path of the directory
where output will occur, but also in filenames.

\param{OutputDirectory}{string} This is where the outputs will be
placed; in particular, the time slices will be here.  The default is
``\$ABACUS\_PERSIST/SimName'', where \$ABACUS\_PERSIST is an environment variable.

\param{LogDirectory}{string} The directory where the logs are to be
stored.  The default is ``\$ABACUS\_PERSIST/SimName/log''.  Each time step
generates several log files, but these all go in the same directory.

\param{LightConeDirectory}{string} The directory where the light
cones should get written.  The default is ``\$ABACUS\_SSD/SimName/''.
The individual LCs will be organized into directorys called ``lc\_rawN'', where N is the number
of the light cone 0..7.
Inside this directory, the outputs will be further divided with one 
subdirectory per time step; these names are fixed to be ``stepNNNN''.
\$ABACUS\_SSD is an environment variable; we default to this fast
directory because we expect to do a merge of these many small files which is hard on the filesystem.
The ``Analysis/LightCones/merge\_lightcones.py'' script will take the
``lc\_rawN'' directory and create a ``lcN'' directory.


\param{GroupDirectory}{string} The directory where group information
will be written.  The default is ``\$ABACUS\_PERSIST/SimName/groups''.  Inside this directory,
the outputs will be further divided with one subdirectory per time
step; these names are fixed to be ``stepNNNN''.
\todo{Not used currently.  Check this after we implement OTF group finding.}


\param{TimeSliceRedshift}{float vector} The redshift of the requested 
time slices.  When redshifts appear in file names, it will be as ``z\%5.3f''.

\param{nTimeSlice}{int} The number of time slice outputs.

\param{NLightCones}{int} The number of light cones.  We support up to 8.

\param{LightConeOrigins}{float vector} The observation point
of the light cones, in triples $(x,y,z)$.  Up to eight light cones 
can be supplied.  The position need not be inside the primary 
wrapping of the box.  The units are the same units as BoxSize.

\param{OutputFormat}{string} Format for the time slice outputs.
Options are: RVdouble, Packed, Heitmann, RVdoublePID.  Packed outputs are described in \S\ref{sec:pack14}.  The other formats are mostly useful for debugging, and their binary formats can be found in ``DataModel/appendarena.cpp''.

\param{BackupDirectory}{string} Location to write intermittent state backups.  Note that this does not need to include the multipoles, since a reliable ``multipole recovery'' utility is available as ``make\_multipoles''.
\todo{Should this also be an environment variable (\$ABACUS\_BACKUP)?}

\param{BackupStepInterval}{int} Number of steps between backups.

\param{SecondIOThreadDirs}{list of string} The directory names whose files will be read/written by the second IO thread.  A typical value would be \verb|["multipole/", "taylor/"]| if the multipoles and Taylors are on a separate disk from the state and would thus benefit from having their IO overlapped with the primary IO thread.  The timings file will identify which directories were assigned to which threads.  The IO logs for each thread will have a ``.1'' or ``.2'' appended to them as appropriate.

\param{nSecondIOThreadDirs}{int} The number of directories in \verb|SecondIOThreadDirs|.

\param{PowerSpectrumStepInterval}{int} The number of steps between computing on-the-fly power spectra.  The 3D density field will be saved in the log directory and then converted to a 1D power spectrum after the step has finished.  The \verb|Analysis/PlotLinearTheory| contains some utilities to make plots from these spectra.

\param{PowerSpectrumN1d}{int} The number of grid cells per dimension to use when computing the OTF density field.

These choices produce the following hierarchy of files:

\fdir{\$ABACUS\_PERSIST/SimName}	(\$ABACUS\_PERSIST is an enviroment variable) \\

\fdir{\$ABACUS\_PERSIST/SimName}		(aka OutputDirectory) \\
\$ABACUS\_PERSIST/SimName/sliceZ.ZZZ/SimName.zZ.ZZZ.slabNNNN.php14

\fdir{\$ABACUS\_PERSIST/SimName/log}	(aka LogDirectory) \\
\$ABACUS\_PERSIST/SimName/log/SimName.stepNNNN.log \\
\$ABACUS\_PERSIST/SimName/log/SimName.stepNNNN.iolog[.1|.2] \\
\$ABACUS\_PERSIST/SimName/log/SimName.stepNNNN.time \\
\$ABACUS\_PERSIST/SimName/log/SimName.stepNNNN.convtime \\
\$ABACUS\_PERSIST/SimName/log/SimName.stepNNNN.convlog

\fdir{\$ABACUS\_PERSIST/SimName/group}	(aka GroupDirectory) \\
\$ABACUS\_PERSIST/SimName/group/stepNNNN/SimName.stepNNNN.slabNNNN.grp

\fdir{\$ABACUS\_PERSIST/SimName/lc\_rawN}	(aka LightConeDirectory + lc\_rawN) \\
\$ABACUS\_PERSIST/SimName/lc\_rawN/stepNNNN/SimName.lcN.stepNNNN.slabNNNN.php14

\fdir{\$ABACUS\_PERSIST/SimName/info}	(misc.~important files) \\

The SimName is used both in the output path and as the file name prefix.
Redshifts are written in names with format \%5.3f.
Slabs and Steps are written with \%04d.
Light cones are written with \%1d.

The php14 suffix is just a place-holder, but this is to indicate the 
file format.

\subsection{Environment Variables}
Some of the above default directories are set by enviroment variables.  These enviroment variables all begin with ``ABACUS\_'' and are optional if you manually specify all directory paths.  The \texttt{install.py} script will help you install these to \texttt{~/.bashrc} or some other suitable location (or set up an Abacus module if on a system that uses modules, like many HPC clusters).  N.B.: these are not parameter file parameters, but instead shell environment variables!

\param{ABACUS}{string} Location of the Abacus source code. Not used by the main simulation code, only by some analysis utilities.

\param{ABACUS\_TMP}{string} Large disk with fast sequential IO for storing state.

\param{ABACUS\_PERSIST}{string} Large, stable disk for storing particle outputs and logs.

\param{ABACUS\_SSD}{string}  Fast, low-latency disk like an SSD for storing multipoles/Taylors and light cones.


\subsection{Cosmological Parameters}

\param{H0}{float} The Hubble constant in km/s/Mpc

\param{Omega\_M}{float} At $z=0$.

\param{Omega\_DE}{float} At $z=0$.

\param{Omega\_K}{float} At $z=0$.

The sum of all the above Omegas must be 1.

\param{w0}{float} Dark energy equation of state $w(z) = w_0 + (1-a)w_a$.

\param{wa}{float} See above.


\subsection{Debugging and Special Cases}\label{sec:special_params}

\param{StoreForces}{int} If 1, store the accelerations in the OutputDirectory.

\param{ForceOutputDebug}{int} If 1, output near and far forces
separately.  The time step is set to zero, so there is no other
action and other outputs (e.g., group finding) may be garbled.
Should only be set if StoreForces is not set.

In addition, using TimeStepDlna=0 forces the next time step to be 0, which
implies that the particles won't advance (although forces and groups
will still be computed).

\param{OutputEveryStep}{int} If $\ge 0$, produce a time slice output at every step.

\subsection{Zel'dovich Initial Condition Generation Parameters}
\param{ZD\_Seed}{int} The random number seed.  This, and ZD\_NumBlock, set the phases of the ICs.

\param{ZD\_NumBlock}{int} This is the number of blocks to break the FFT
into, per linear dimension.  This must be an even number; moreover,
it must divide PPD (NP$^{1/3}$) evenly.  The default is 2, but you
may need a higher number.  Changing ZD\_NumBlock will change the
phases of the ICs for a given ZD\_Seed.\todo{Make phases independent
of ZD\_NumBlock}

This is a key tuning parameter for the code.  The full problem
requires $32\times NP$ bytes, which may exceed the amount of RAM.
The zeldovich code holds $2/{\rm NumBlock}$ of the full volume in memory,
by splitting the problem in 2 dimensions into NumBlock$^2$ parts.
Each block therefore is $32\times NP/{\rm NumBlock}^2$ bytes.  It
is important that these blocks be larger than the latency of the
disk, so sizes of order 100 MB are useful.  We are holding
$2\times$NumBlock such blocks in memory.

Hence, for a computer with $M$ bytes of available memory and a
problem of $NP$ particles, we need NumBlock $>64\times NP/M$
(preferably to be the next larger even number that divides evenly
into $NP^{1/3}$) and we prefer that $32 NP/{\rm NumBlock}^2$ is 
larger than the latency.

For example, for a $4096^3$ simulation, $M$ is 2 TB.  If we use
NumBlock of 128, then we will need 32 GB of RAM and each block saved
to disk will be 128 MB.  For a $2048^3$ simulation, $M$ is 256 GB
and NumBlock of 16 will require 32 GB of RAM and each block will
be 1024 MB.  For a $8192^3$ simulation, $M$ is 16 TB and NumBlock
of 512 will require 64 GB of RAM and a block size of 256 MB.

If ZD\_k\_cutoff $\ne 1$, then the actual ZD\_NumBlock will be ZD\_NumBlock*ZD\_k\_cutoff.
See ZD\_k\_cutoff for details.


\param{ZD\_Pk\_filename}{string} The file name of the input power spectrum.  This can be a CAMB power spectrum.

\param{ZD\_Pk\_scale}{double} This is the quantity by which to multiply the 
wavenumbers in the input file to place them in the units that will be used
in the zeldovich code, in which the fundamental wavenumber is $2\pi$ divided
by BoxSize.  Default value is 1.0.

As a common example, one might need to convert between Mpc$^{-1}$ and
$h$~Mpc$^{-1}$ units.  The zeldovich code does not use the hMpc value,
so it doesn't know what the units of BoxSize are.  If BoxSize is in 
$h^{-1}$~Mpc units, so that hMpc=1, and if the $P(k)$ file had $k$
in $h$~Mpc$^{-1}$ units, then all is well: use the value of 1.0.

However, if BoxSize were in Mpc units and the input power were in 
$h$~Mpc$^{-1}$ units, then we want to convert the wavenumbers to 
Mpc$^{-1}$ units.  That means multiplying by $h$, so we should use
ZD\_Pk\_scale $=h$.

\param{ZD\_Pk\_norm}{double} The scale at which to normalize the input
$P(k)$, in the same units as BoxSize.  For example, if BoxSize is
given in $h^{-1}$~Mpc, then one might choose 8 to select $\sigma\_8$.
If this value is 0, then the power spectrum will not be renormalized
(but ZD\_Pk\_scale will be applied to the wavevectors, so beware that
the power isn't thrown off, as it does have units of volume).  The
default is 0, but we recommend controlling the normalization.

\param{ZD\_Pk\_sigma}{double} The amplitude to use to normalize the
fluctuations of the density field, with a tophat of radius ZD\_Pk\_norm.
This must be scaled to the initial redshift by the growth function; 
the zeldovich code does not know about cosmology.  The default is 0,
but one almost certainly wants to change this!

The Abacus Python wrapper will compute this value based on the cosmology and \verb|sigma_8|.  You almost never want to specify \verb|ZD\_Pk\_sigma|; instead specify \verb|sigma_8|.

Note that using this parameter means that the choice of unit of power 
in the input power spectrum file is irrelevant (but we do care about
the unit of wavenumber, see above).

\param{sigma\_8}{double} The amplitude of present-day density fluctuations $\sigma_8$.  This value is not read by the zeldovich code, but instead processed by the Python wrapper to produce \verb|ZD\_Pk\_sigma| by scaling via the growth function.

\param{ZD\_Pk\_smooth}{double} The length scale by which to smooth the input 
power spectrum before generating the density field.  This is applied as
a Gaussian smoothing as $\exp(-r^2/2a^2)$ on the density field, which is
 $\exp(-k^2 a^2)$ on the power spectrum.  Smoothing the power spectrum 
is useful for testing, as it reduces grid artifacts.  The smooth occurs
after the power spectrum has been normalized.  Default is 0.

\param{ZD\_qoneslab}{int} If $\ge0$, output only one PPD slab.  For debugging only.
The default is $-1$.

\param{ZD\_qonemode}{int} If $\ge0$, zero out all modes except the one with the wavevector specified in ZD\_one\_mode.

\param{ZD\_one\_mode}{three ints} This is the one wavevector that will be inserted into the box if ZD\_qonemode $\ge0$.
This is useful for automatically iterating through a series of wavevectors for examining isotropy or Nyquist effects, for example.
Each component can be an integer in the range [-ppd/2,ppd/2].

\param{ZD\_qPLT}{int} If $\ge0$, turn on particle linear theory corrections.
This tweaks the displacements and velocities, mostly near $k_{\rm Nyquist}$, to ensure everything starts in the growing mode.
The output format will include velocities if you turn this on, either in the RVZel or RVdoubleZel format (set by a Makefile flag).
PLT features are described in Garrison, et al. (2016).

\param{ZD\_PLT\_filename}{string} The file containing the PLT eigenmodes; i.e.~the true growing modes for the grid.
We generate this on something like a $128^3$ grid and linearly interpolate the eigenmodes and eigenvalues as needed.

\param{ZD\_qPLT\_rescale}{int} If $\ge0$, increase the amplitude of the displacements on small scales (near $k_{\rm Nyquist}$)
to compensate preemptively for future undergrowth that we know happens on a grid.

\param{ZD\_PLT\_target\_z}{int} If ZD\_qPLT\_rescale $\ge0$, then increase the initial displacements such that they will match the linear theory prediction
at this redshift.  Recall that modes on the grid (mostly) grow more slowly than linear theory, which is why we increase the initial displacements.
This redshift should be before shell-crossing while linear theory is still mostly valid, e.g.~$z\sim 5$.
\todo{Provide a heuristic for calculating this number based on pixel-level density fluctuations}

\param{ZD\_k\_cutoff}{double} The wavenumber above which not to input any power, expressed such that $k_{\rm max} = k_{\rm Nyquist} / k_{\rm cutoff}$, e.g.~ZD\_k\_cutoff = 2 means we null out modes above half-Nyquist.  Non-whole numbers like 1.5 are allowed.  This is useful for doing convergence tests, e.g.~run once with PPD=64 and ZD\_k\_cutoff = 1, and again with PPD=128 and ZD\_k\_cutoff = 2.  This will produce two boxes with the exact same modes (although the PLT corrections will be slightly different), but the second box's modes are oversampled by a factor of two.  To keep the random number generation synchronized between the two boxes (fixed number of particle planes per block), ZD\_NumBlock is increased by a factor of ZD\_k\_cutoff.

\bigskip

In addition, the Zel'dovich code expects the following parameters, 
defined above.

\param{BoxSize}{double} This is the box size, either in Mpc or $h^{-1}$~Mpc,
depending on the setting of the hMpc key.  However, the zeldovich code does
not use the hMpc value.  See ZD\_Pk\_scale for further discussion.

\param{NP}{long long int} This must be a perfect cube for the zeldovich code to work.

\param{CPD}{int} Zeldovich will output in slabs.  These slabs are only defined by the 
initial grid position; we do not guarantee that the initial displacement may not have
taken the particle out of the slab.  Usually the deviations will be small enough that 
particles move by at most 1 slab.

\param{InitialConditionsDirectory}{string} In addition to the usual usage,
the zeldovich code will use this for the swap space for the block transpose.
This will generate files of the name `zeldovich.\%d.\%d'.  These will be deleted
after the code has run.

\param{InitialRedshift}{double}  Starting redshift of the sim; i.e.~the output redshift for the zeldovich code.  Only used by the zeldovich code for PLT rescaling, although the Python wrapper will use it to compute the growth function.

\bigskip

The following options have been disabled in the current code:

\param{ZD\_density\_filename}{string} The file name where to write the density
field.
\todo{This option may be disabled.}

\param{ZD\_qdensity}{int} If non-zero, output the grid densities that generate
the Zel'dovich displacements.  Default is 0.
\todo{This option may be disabled, but I think that we should eventually restore 
this feature.}

\todo{Perhaps we make the output of densities standard?  E.g., four floats?}

\clearpage

\section{State file formats}

A State is actually a full sub-directory, with all of the files
describing the simulation's particles.

The file called `state' is an ASCII file in ParseHeader format.  It
is described in the next section.  This file is created last: if
it exists, then the code singlestep completed properly and the state
is ready to be used (read-only) by the next time step of abacus.
If it does not exist, then the state is invalid.  singlestep will
not proceed if the WriteState state file exists, since it presumes
it would overwriting valid data.  However, deleting only the state
file will allow singlestep to run, using the read state and the
write state Taylors, but overwriting all else.

Many quantities are stored in slabs, implying CPD files each.  These
include the positions, velocities, auxillary variables, cellinfo
structures, multipoles, and Taylors.  Notably positions and velocities
are stored as float[3] or double[3] arrays (so one particle's x,
y, and z, then the next, etc.).  Multipoles and Taylors are stored
in YZ Fourier space, so these rather cryptic.  Auxillary and cellinfo
structures are defined in the code.

There are also a few smaller files.  `slabsize' is ASCII and holds
the number of particles in each slab (after opening with CPD).  The
`redlack\_\{x,y,z\}' and globaldipole files contain information for
the Redlack-Grindlay term.

Accelerations can also be written on request (see \S\ref{sec:special_params}).

\section{State File Description}

The State file contains information that is computed by the Abacus
code, particularly values that are epoch-dependent.

\subsection{Properties of the Simulation}

\param{np\_state, cpd\_state, order\_state}{int} These are just
copied from the parameter file, so that we can be sure to be able
to read the state files.

\param{ParameterFileName}{string} The name of the Parameter file
on which the state was invoked.

\param{CodeVersion}{string} The git hash label of the executable.

\param{RunTime}{string} The time stamp of the start of this invocation
of singlestep().

\param{MachineName}{string} The machine name where this step is
being run.

\param{DoublePrecision}{int} Whether the internal positions,
velocities, and forces are being computed in single precision (==0)
or double precision (==1).

\param{ppd}{double} The cube root of the number of particles.  We
take care to round this off to an integer if it is very close.

\param{FullStepNumber}{int} The full step number.  The initial
conditions get translated into a state (with multipoles) that is
defined as step number 0.  So the full step number is also the
number of times that full forces have been computed.

\param{LPTStepNumber}{int} How many steps into our Lagrangian Perturbation Theory computation we are.  0 if this is a normal time step; non-zero if this is an LPT step.

\param{SofteningType}{string} The force law being employed.  Possible
values are: ``plummer'', ``cubic\_plummer'', ``cubic\_spline'', ``single\_spline''.

\param{SofteningLength}{double} The Plummer-equivalent softening length
that the user inputs.  See the description in the Parameters section.
Same units as BoxSize.

\param{SofteningLengthInternal}{double} The softening length scaled
for this particular softening technique to produce a minimum orbital
period that matches Plummer.  Same units as BoxSize.


\subsection{Computed Properties of the Cosmology}

\param{BoxSizeMpc, BoxSizeHMpc}{double} The comoving size of the
box, in Mpc and $h^{-1}$ Mpc.

\param{HubbleTimeGyr, HubbleTimeHGyr}{double} The value of $1/H_0$
in Gyr and $h^{-1}$ Gyr.  This is important because all of the times
below will be reported in units of $H_0=1$.

\param{ParticleMassMsun, ParticleMassHMsun}{double} The mass of a
single particle, in $M_\odot$ and $h^{-1}\ M_\odot$.

\subsection{Properties of the Epoch}

\param{ScaleFactor}{double} The scale factor $a$, normalized to $a=1$ 
at the present day.

\param{Redshift}{double} The redshift $1+z = 1/a$.

\param{VelZSpace\_to\_kms}{double} The Hubble velocity across
the full box in km/s.  In other words, this is $H(z)L/(1+z)$, in
km/s.  If one has velocities in redshift-space displacement unit-box 
units (our default), then multiplying by this factor will convert the
velocity to km/s.

\param{VelZSpace\_to\_Canonical}{double} The conversion from our basic
output velocity unit of redshift-space displacement (in unit box
length units) to the code canonical velocities. 
We have $v_{\rm canon} = a v_{\rm proper} = a^2 v_{\rm comoving}$
and $v_{\rm proper} = H(z) a v_{\rm zspace}$, so
$v_{\rm canon} = v_{\rm zspace} a^2 H(z)/H_0$ (remembering that
the code uses $1/H_0$ time units).

\param{Time}{double} The proper time, in $1/H_0$ units.

\param{etaK}{double} The integral $\int_0^t dt/a$, which is used
in kicking the particles.  This is also the conformal time.  In
$1/H_0$ units.

\param{etaD}{double} The integral $\int^t dt/a^2$, which is used
in drifting the particles.  In $1/H_0$ units.  This integral diverges
as $a\rightarrow0$, but we initialize at an early time with the 
quantity $\eta_D = -2/a^2 H(z)$.  Although $\eta_D$ is negative,
it increases with time and our $\Delta\eta_D$ are positive.

\param{Growth}{double} The linear growth function, normalized to $a$ at early times.

\param{Growth\_on\_a}{double} The linear growth function divided by
$a$, which is unity in Einstein-de Sitter.

\param{f\_growth}{double} The growth rate $d\ln D/d\ln a$.

\param{w}{double} The equation of state of dark energy at the present epoch.

\param{HubbleNow}{double} The Hubble parameter at the current epoch,
in $H_0$ units, i.e., $H(z)/H_0$.

\param{Htime}{double} The product of the Hubble parameter and the
current time, $H(z) t(z)$.  This is 2/3 in Einstein-de Sitter.

\param{OmegaNow\_m}{double} The value of $\Omega_m$ that an observer
at this epoch would measure.

\param{OmegaNow\_K}{double} The value of $\Omega_K$ that an observer
at this epoch would measure.

\param{OmegaNow\_DE}{double} The value of $\Omega_{DE}$ that an
observer at this epoch would measure.

\subsection{Properties of the Timestep}

\param{DeltaTime}{double} The time difference between this state
and the previous one.

\param{DeltaScaleFactor}{double} The scale factor difference between
this state and the previous one.

\param{DeltaRedshift}{double} The redshift difference between this
state and the previous one (flipped to be a positive number for increasing
time).

\param{ScaleFactorHalf}{double} The scale factor of the time halfway
between this state and the previous one.

\param{LastHalfEtaKick}{double} The $\Delta\eta_K$ kick that remains
to be applied to the velocities in this state to bring them up to
the epoch in this state.  Recall that the velocities are stored a 
half-step before and will be leaped over to be a half-step ahead, 
before they are drifted again.  Note that when the particles are in
the Read state, this is the $\Delta\eta_K$ we will apply, yet we 
no longer have access to the state epoch {\it before} this Read state,
hence the need to save this quantity.

\param{FirstHalfEtaKick}{double} The $\Delta\eta_K$ kick that will
be applied to move the velocities halfway from the previous epoch
to the current epoch.

\param{DeltaEtaDrift}{double} The $\Delta\eta_D$ drift that will
be applied to move the positions from the previous epoch
to the current epoch.

\subsection{Statistics of the Particle Distribution}
These are used to compute the time step and to monitor the code.

\param{RMS\_Velocity}{double}
The rms velocity in the entire simulation ($\left<|\vec{v}|\right>$),
in simulation units (canonical velocities, unit box length, $1/H_0$
time unit).  One can use the VelZSpace\_to\_Canonical and
RedshiftSpaceConversion values to convert the units.  Important:
this is the rms velocity at the epoch in ReadState: the velocities
in the ReadState are kicked to be synchronous with the positions,
then this maximum is tracked and written into WriteState.

\param{MaxVelocity}{double} The maximum velocity component
($v_{\{x,y,z\}}$), in simulation units (canonical velocities, unit
box length, $1/H_0$ time unit).  One can use the VelZSpace\_to\_Canonical
and RedshiftSpaceConversion values to convert the units.  Note that
this is not the maximum $|\vec{v}|$.  Important: this is the maximum
velocity at the epoch in ReadState: the velocities in the ReadState
are kicked to be synchronous with the positions, then this maximum
is tracked and written into WriteState.

\param{MaxAcceleration}{double} The maximum acceleration component
($a_{\{x,y,z\}}$), in simulation units (unit box length, $1/H_0$
time unit).  Note that this is not the maximum $|\vec{a}|$.  Important:
this is the maximum acceleration at the epoch in ReadState: the
positions in the ReadState are used to compute accelerations, on
which this maximum is tracked and written into WriteState.

\param{MinVrmsOnAmax}{double}
In each cell, we compute the ratio of the rms velocity and the
maximum acceleration.  The rms velocity is $\left<|\vec{v}|^2\right>$,
i.e., the mean velocity has not been removed before computing the
second moment, so this is not Galilean invariant.  The maximum
acceleration is done by component ($a_{\{x,y,z\}}$).  After this
ratio is computed for each cell, the minimum over all cells is
computed and reported.

\todo{Reconsider whether computing the full norms or proper variances 
would actually be expensive.}

\param{MaxCellSize, MinCellSize}{int} The maximum and minumum number
of particles in a cell.  These are computed at the positions in
WriteState.

\param{StdDevCellSize}{double} The standard deviation of the density contrast (fractional overdensity $\rho/\bar\rho - 1$) within cells.


\clearpage
\section{Output Files}

Abacus can produce both time slice and light cone files.  

Time slices are divided into CPD slab files, for reasonable
portability.  These files are all written into one directory, with
a different directory for each time slice.

Light cones span many steps.  To keep the files organized, we write
one directory per time step, with CPD slab files in each directory.
Of course, these files will be notably smaller than the time slice
files (only a fraction of particles in each slab are output in a
given time step).  But since we are reading and writing a half-dozen
large files per slab per step, writing one more, even if small, is
negligible overhead.

For both types of output, we write a ParseHeader format header at
the top of the file.  The Parameter file is in this header, as is
a large portion of the State variables (those not known at the time
of output are omitted).  After the ParseHeader end-of-header token,
the particles, velocities, and perhaps auxillary variables are written
in a binary format.

In addition, the power spectrum file used in the Zel'dovich code
is copied into the Output and Log directory when the ICs are created.

Sometimes we have many auxillary files that we want to carry around
with the outputs.  An example would be CAMB parameter files or simple
diagnostic plots like the power spectrum evolution.  These can be placed
in the \verb|info| directory in the simulation configuration directory;
the Python wrappers will copy this info directory to any relevant
output or data product directories.

\section{Packed Outputs (pack14)}\label{sec:pack14}

The cell-oriented structure of abacus allows for an efficient way
to compress the particle data.  We output particles in sets for a
given cell, and the positions are stored relative to the cell center.
A typical cell size is a few Mpc.  12 bits of precision will yield
a part in 4000 precision, which is about 1 kpc.  This is adequate
for all reasonable output analysis tasks.  Note that abacus output
files are not intended to be adequate for dynamical restarts; one
always has the option to save states for restart.

Abacus always outputs its velocities in redshift-space units, i.e.,
the comoving displacement that the particle would appear to be in
redshift space.  This avoids a need to choose a second set of units
for velocities.  We have $v_{\rm proper} = Hx_{\rm proper} =
Hax_{\rm comoving}$, so $v_{\rm zspace} = v_{\rm proper}(1+z)/H(z)$.

Velocities rarely get above 6000 km/s, so a part in 4000 means 1-2
km/s precision, which is 10 kpc in zspace displacement.  Again, this
is adequate for output analyses.  We track the maximum velocity 
in unit-cell lengths and rescale the velocities by a factor $A$ 
so that their displacements are always contained within the unit cell.

So our standard pack14 class stores 12 bits for each of the 6 phase
space components, a total of 9 bytes.  We then add 5 bytes of ID
numbers or group IDs, for a total of 14 bytes per particle.  We call
this the pack14 format.

The 12 bits are stored as unsigned.  However, phase space quantities
are signed, so we add 2048 before storing.  We scale the unit cell 
to a range of $\pm2000$.  Phase space quantities are permitted to 
be in the range of $\pm2030$, to allow some small overflow beyond 
the cell.

The id numbers are stored explicitly as unsigned characters, so
that the standard is endian-independent.

Cells are begun with a structure of the same 14-byte length.  This
structure can be identified because the first byte is 0xff, which
makes the first position value 2032-2047.  The remaining 5
12-bit numbers are filled with the cell $i$, $j$, $k$, the CPD 
of the calculation, and the velocity rescaling $A$ (which must be
a positive integer).  Note that all of these quantities are notionally
unsigned, but it might be convenient to allow the  $ijk$ to go slightly
negative (e.g., for a periodic wrap).  Therefore, we add 30 to these
quantities before storing.  Values outside of 16..4080 are still illegal, 
so these values must lie between $-30$ and 4020.  Hence, the standard
only works to CPD of 4020.  The 5 bytes for ID are currently unused
in the cell headers.

After a cell element, all elements are taken to be particles in
that cell, until the next cell element is read.  We do not insist
on knowing the number of particles in the cell before writing the
cell header.  Since we are likely to have dozens to hundreds of
particles per cell, the cost of the cell header is small.

The global position is then (cellx + particlex/2000.0)*B, where B
is the box size divided by CPD.  

The global velocity is (particlevx/2000.0)*A*B, where A is the
scaling factor chosen when packing the velocities.  We expect
that good A's will be of order 30.  There is not much value in 
making A smaller than 1, since this would mean more resolution 
in velocity than position, so we store $A$ as an integer. 

We provide routines to unpack these files to double-precision lists of
positions and velocities.  Note that 12 bits intra-cell, plus 10-12 bits
of cell ids implies 22-24 bits of precision, comparable to single precision.

Note that were the data stored in single precision, we would require
24 bytes for the phase space data, plus 5-8 bytes for ids, a total of 
29-32 bytes.  So the packing saves a factor of 2 in disk space.

\section{Logging}

Abacus produces an extensive set of log files for each simulation.
These are written into the log directory, with separate files for
each time step. 

\todo{In addition, there is a top-level summary log that is updated
after each successful time step.}

The singlestep code produces 2 to 4 logs: a primary log, a report
on timings, and a log on the actions of the I/O thread(s), if those
threads are running.  The convolution code produces a primary log
and a timing log.

The log contains a millisecond-level record of the elapsed time
since the log was initiated.  Log verbosity can be controlled with
a parameter choice.

If the program has crashed, then the files laststep.\{suffix\} contains
the logs.

\begin{private}
\section{Build System}
Abacus uses an Autoconf-based \texttt{configure}/\texttt{make} build system.
When building Abacus for the first time, use
\begin{verbatim}
./configure
make
\end{verbatim}
The \verb|./configure| step will generate a \verb|Makefile| from each \verb|Makefile.in|
and print a summary of the Abacus code options
that were selected.  Running make again will build Abacus with those same code
options until \verb|./configure| is run again, or until you run \verb|make distclean|.

Use \verb|./configure --help| to see the various build options.  In particular,
the \verb|Optional Features| and \verb|Optional Packages| contain most of the code
options.  To compile Abacus with the Intel compiler, the synatx is \verb|./configure CXX=icc|.
\todo{There's a fair number of unused options in ./configure --help.  Unclear
how to get rid of them.}

\verb|make analysis| will only build analysis packages and not the main simulation
code.  Useful because the compiling singlestep can be slow.

Most of the code is built as a single compilation unit via \verb|#include|s instead
of as separate object files that are linked.  This leads to longer compilation times,
but we rely on the agressive interprocedural optimizations that this allows.
The exception is the GPU direct code \verb|gpudirect.a|, which is built as a separate
static library by \verb|nvcc| and linked to the singlestep code.

The build system can also create another binary called \verb|recover_multipoles| via
\verb|make recover_multipoles|.  This is a redacted singlestep pipeline that simply loads positions
and cell infos and generates the corresponding multipoles.  Useful for recovering from
backups or lost/corrupted multipoles.  You probably never have to invoke this manually;
the Python wrapper will invoke it as necessary.
\todo{This could probably be a command line flag to singlestep, rather than a
separately compiled executable.}

\end{private}

\begin{private}
\section{Analysis Codes}
Some generally useful analysis utilities are included in the main code repository (primarily in \verb|Analysis/|,
but also some useful utilities in \verb|Abacus|).
They are largely stand-alone and documented inside their own directories; this section
just serves as an overview.

All of the utilities are designed to operate on Abacus outputs in pack14 format.  They
will also try to create a useful directory structure for any data products.  Some of the
utilities also support automatic tar ball creation to facilitate data product distribution.
In general, use \verb|{./utility.py} --help| to see the available options.

\subsection{FoF (\texttt{Analysis/FoF})}
Friends-of-friends halo finding. Supports a subhalo detection by running FoF again with a smaller
linking length.  Output catalogs include particle subsamples and PIDs.

\subsection{Rockstar (\texttt{Analysis/Rockstar})}
The standard Rockstar halo finder with support for Pack14.  Also produces particle subsamples.

\subsection{Light Cones (\texttt{Analysis/LightCones})}
Currently only contains a utility for merging the individual LC files into slabs.

\subsection{Power Spectrum (\texttt{Analysis/PowerSpectrum})}
A full Python-based power spectrum stack.  Takes particles (either from disk or memory),
produces a density field, does the FFT, and bins in spherical annuli to produce a 1D power spectrum.
Many other options/modes of operation related to density fields.

Also contains scripts for histogramming and computing Gaussian 2PCF covariance matrices.


\subsection{Halo Interfaces (\texttt{Abacus/[Halos.py|Halotools.py]})}
Interfaces to wrangle the various halo catalogs Abacus produces (via FoF or Rockstar).
\texttt{Halos.py} is pure Python; \texttt{Halotools.py} is just a wrapper that puts
everything in Halotools format.

\subsection{Abacus IO (\texttt{Abacus/ReadAbacus.py})}
Interfaces to deal with the various Abacus particle formats.

\end{private}

\end{document}
